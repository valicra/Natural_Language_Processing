{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def import_corpus(path_to_file):\n",
    "    \"\"\"\n",
    "    :param path_to_file: Path to corpus file\n",
    "    :return: List of list of tuples, e.g. [('A', 'DT'), ('Lorillard', 'NNP'), ('spokewoman', 'NN'), ...\n",
    "    \"\"\"\n",
    "    with open(path_to_file) as f:\n",
    "        return [[tuple(pair.split(' ')) for pair in sent.strip().split(\"\\n\")] for sent in f.read().split(\"\\n\\n\")\n",
    "                if len(sent) > 0]\n",
    "\n",
    "def get_key_from_value(my_dict, target_value):\n",
    "    for key, value in my_dict.items():\n",
    "        if value == target_value:\n",
    "            return key\n",
    "    return None\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def c_log(x):\n",
    "    \n",
    "    max_float=np.finfo(float).max\n",
    "    min_float= np.finfo(float).eps\n",
    "    \n",
    "    if x < min_float:\n",
    "        x = min_float\n",
    "\n",
    "    if x > max_float:\n",
    "        x = max_float  \n",
    "\n",
    "    result = np.log(x)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "class LinearChainCRF(object):\n",
    "    def __init__(self, corpus):\n",
    "        # special label indicating start of sentence\n",
    "        self.START_LABEL = '<start>'\n",
    "\n",
    "        # Corpus: [[(Word, Label), (Word, Label), ...],[(Word, Label),(Word,Label),...],...]\n",
    "        random.shuffle(corpus)\n",
    "        self.corpus = corpus\n",
    "\n",
    "        # create train/test split\n",
    "        num_train_sentences = int(len(corpus) * 0.1) # only use a small fraction of sentences for efficiency\n",
    "        self.train_sentences = corpus[:num_train_sentences]\n",
    "        self.dev_sentences = corpus[num_train_sentences:]\n",
    "\n",
    "        # Create a set of all labels whose count is bigger than Min label count\n",
    "        self.labels = list({label for s in self.train_sentences for token, label in s})\n",
    "        self.tokens = list({token for s in self.train_sentences for token, label in s})\n",
    "\n",
    "        self.labels.append(self.START_LABEL)\n",
    "\n",
    "        # create the dict feature_indices (Feature to ID)\n",
    "        self.label_indices = {label: index for index, label in enumerate(self.labels)}\n",
    "        self.token_indices = {token: index for index, token in enumerate(self.tokens)}\n",
    "\n",
    "        # initialize theta with 1s (each feature is weighted the same initially)\n",
    "        self.theta = np.ones((len(self.labels), len(self.labels) + len(self.tokens)))\n",
    "\n",
    "        # compute empirical feature count initialize with 0s\n",
    "        self.empirical_count = np.zeros_like(self.theta)\n",
    "         \n",
    "\n",
    "        # TODO: Exercise 1e - Precompute the empirical feature count\n",
    "        #  For each occurrence of a feature in the training data, the corresponding index in self.empirical_count\n",
    "        #  should be increased by 1\n",
    "        \n",
    "\n",
    "        for s in self.train_sentences:\n",
    "            for i,(token,label) in enumerate(s):\n",
    "                if i==0:\n",
    "                    self.empirical_count[self.label_indices[label], self.label_indices[self.START_LABEL]]+=1 # update start|l\n",
    "\n",
    "                else:\n",
    "                    self.empirical_count[self.label_indices[label], self.label_indices[s[i-1][1]]]+=1 # update l-1|l\n",
    "                \n",
    "                self.empirical_count[self.label_indices[label], len(self.labels) + self.token_indices[token]]+=1 # update l|t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def psi(self, label, prev_label, token):\n",
    "        \"\"\"\n",
    "        :param label: string with the current label\n",
    "        :param prev_label: string with the previous label\n",
    "        :param token: string with the current token\n",
    "        :return: float for psi\n",
    "        \"\"\"\n",
    "        # TODO: Exercise 1a - Compute psi using the definition of the features from the exercise sheet and self.theta\n",
    "        \n",
    "        \n",
    "        if token not in self.token_indices:\n",
    "            token = get_key_from_value(self.token_indices, np.argmax(self.empirical_count[self.label_indices[label]])) # get the most frequent token given the label \n",
    "        \n",
    "        ll_index= (self.label_indices[label], self.label_indices[prev_label]) # label| prev_label index\n",
    "        lt_index= (self.label_indices[label], len(self.label_indices) + self.token_indices[token]) # label|token index\n",
    "        \n",
    "        psi = self.empirical_count[ll_index] * self.theta[ll_index] + self.empirical_count[lt_index] * self.theta[lt_index]\n",
    "    \n",
    "\n",
    "        return psi\n",
    "\n",
    "\n",
    "    def forward_variables(self, sentence):\n",
    "        \"\"\"\n",
    "        :param sentence: An input to compute the forward variables/alpha on.\n",
    "        :return: Data structure containing the matrix of forward variables.\n",
    "        \"\"\"\n",
    "        # TODO: Exercise 1b - Compute the forward variables using your implementation of psi\n",
    "        #  We recommend to return a 2D array of the shape (len(sentence), len(self.labels))\n",
    "\n",
    "        alpha = np.zeros((len(sentence),len(self.labels)))\n",
    "        \n",
    "        for i, label in enumerate (self.labels):\n",
    "            alpha[0,i] = self.psi(label, self.START_LABEL, sentence[0][0])\n",
    "        \n",
    "        for w, (token, _ ) in enumerate(sentence[1:]): # remember that enumerate strats from 0. here i fix the token. w = word index \n",
    "            for j, label in enumerate(self.labels): # fix the label j = label index\n",
    "                for i, prev_label in enumerate(self.labels): # loop over prev_labels i= looping index \n",
    "                    \n",
    "                    try:\n",
    "                        alpha[w+1,j]+= self.psi(label, prev_label, token) * alpha[w,i]\n",
    "                    except RuntimeWarning:\n",
    "                        alpha[w+1,j] = np.finfo(float).max\n",
    "                        continue\n",
    "                \n",
    "                   \n",
    "        #replacement_value_inf = np.finfo(float).max\n",
    "        #replacement_value_neginf = np.finfo(float).eps\n",
    "\n",
    "        # Substitute inf and -inf with the respective replacement values\n",
    "        #alpha_no_inf = np.where(np.isinf(alpha), replacement_value_inf,\n",
    "                                #np.where(np.isneginf(alpha), replacement_value_neginf, alpha))\n",
    "\n",
    "\n",
    "        #normalized_alpha= alpha_no_inf / np.sum(alpha_no_inf, axis=1, keepdims=True)\n",
    "        \n",
    "        #print(normalized_alpha)\n",
    "        return alpha\n",
    "\n",
    "    def backward_variables(self, sentence):\n",
    "        \"\"\"\n",
    "        :param sentence: An input to compute the backward variables/beta on.\n",
    "        :return: Data structure containing the matrix of forward variables.\n",
    "        \"\"\"\n",
    "        # TODO: Exercise 1b - Compute the backward variables using your implementation of psi\n",
    "        #  We recommend to return a 2D array of the shape (len(sentence), len(self.labels))\n",
    "\n",
    "        beta = np.zeros((len(sentence),len(self.labels)))\n",
    "\n",
    "        for i in range(len(self.labels)):\n",
    "            beta[len(sentence)-1, i] = 1 \n",
    "\n",
    "        sentence = list(reversed(sentence))\n",
    "        for w, (token, _ ) in enumerate(sentence[1:]): # remember that enumerate strats from 0. here i fix the token. w = word index (row)\n",
    "            for i,prev_label in enumerate(self.labels): # fix the prev_label i = column \n",
    "                for j, label in enumerate(self.labels): # loop over labels j = looping index\n",
    "                    try:\n",
    "                        beta[len(sentence)-1-(w+1),i]+= self.psi(label, prev_label, token) * beta[len(sentence)-1-w,j]\n",
    "                    except RuntimeWarning:\n",
    "                        beta[len(sentence)-1-(w+1),i]=np.finfo(float).max\n",
    "                        continue\n",
    "   \n",
    "                    \n",
    "        \n",
    "        #replacement_value_inf = np.finfo(float).max\n",
    "        #replacement_value_neginf = np.finfo(float).eps\n",
    "\n",
    "        # Substitute inf and -inf with the respective replacement values\n",
    "        #beta_no_inf = np.where(np.isinf(beta), replacement_value_inf,\n",
    "         #                       np.where(np.isneginf(beta), replacement_value_neginf, beta))\n",
    "\n",
    "\n",
    "        #normalized_beta= beta_no_inf / np.sum(beta_no_inf, axis=1, keepdims=True)\n",
    "        \n",
    "        #normalized_beta= beta / np.sum(beta, axis=1, keepdims=True)\n",
    "        return beta\n",
    "\n",
    "    def compute_z(self, sentence, alpha_beta):\n",
    "        \"\"\"\n",
    "        :param sentence: A sentence to compute the partition function Z on\n",
    "        :param alpha_beta: Your alpha or beta variables\n",
    "        :return: float - Result of the partition function Z\n",
    "        \"\"\"\n",
    "        # TODO: Exercise 1c - Compute the partition function using a datastructure with either\n",
    "        #  your alpha or beta variables\n",
    "\n",
    "        alpha=self.forward_variables(sentence)\n",
    "        z= np.sum(alpha_beta[-1, :])\n",
    "        return z\n",
    "\n",
    "    def marginal_probability(self, sentence, t, y_t, y_t_minus_one, alpha, beta, Z):\n",
    "        \"\"\"\n",
    "        Compute the marginal probability of the labels given by y_t and y_t_minus_one given a sentence.\n",
    "        :param sentence: list of strings representing a sentence.\n",
    "        :param t: position in sentence for marginal probability, 0-based\n",
    "        :param y_t: element of the set 'self.labels'; label assigned to the word at position t\n",
    "        :param y_t_minus_one: element of the set 'self.labels'; label assigned to the word at position t-1\n",
    "        :param alpha: data structure holding the current alpha variables of the sentence\n",
    "        :param beta: data structure holding the current beta variables of the sentence\n",
    "        :param Z: current z value\n",
    "        :return: float: probability;\n",
    "        \"\"\"\n",
    "        # TODO: Exercise 1d - Compute the marginal probability using the datastructures from forward\n",
    "        #  and backward, as well as the psi function.\n",
    "\n",
    "        p = (alpha[t-1,self.label_indices[y_t_minus_one]]*self.psi(y_t, y_t_minus_one, sentence[t][0]) * beta[t,self.label_indices[y_t]] )/ Z\n",
    "        \n",
    "        return p\n",
    "\n",
    "    def expected_feature_count(self, sentence):\n",
    "        \"\"\"\n",
    "        :param sentence: Sentence to compute the expected feature count on\n",
    "        :return: Data structure holding the expected feature count for each feature\n",
    "        \"\"\"\n",
    "        # TODO: Exercise 1f - Compute the expected feature count for a sentence. We recommend to return a data structure\n",
    "        #  with the same shape as self.theta. It is given that alpha, beta and Z should first be computed\n",
    "        alpha = self.forward_variables(sentence)\n",
    "        beta = self.backward_variables(sentence)\n",
    "        Z = self.compute_z(sentence, alpha)\n",
    "        \n",
    "    \n",
    "\n",
    "        expected_count=np.zeros_like(self.empirical_count)\n",
    "        \n",
    "        for i, (token, label) in enumerate(sentence):\n",
    "                for prev_label in self.labels:\n",
    "                    \n",
    "                    marginal_p= self.marginal_probability(sentence, i,label, prev_label, alpha, beta, Z)\n",
    "                    \n",
    "                    expected_count[self.label_indices[label], self.label_indices[prev_label]] = (\n",
    "                        self.empirical_count[self.label_indices[label], self.label_indices[prev_label]]* marginal_p\n",
    "                        )\n",
    "                    expected_count[self.label_indices[label], len(self.label_indices) + self.token_indices[token]] = (\n",
    "                        self.empirical_count[self.label_indices[label], len(self.label_indices)+self.token_indices[token]] * marginal_p\n",
    "                    )\n",
    "        return expected_count\n",
    "\n",
    "    def train(self, num_iterations, learning_rate=0.01, evaluate_after=20):\n",
    "        \"\"\"\n",
    "        :param num_iterations: Number of training iterations\n",
    "        :param learning_rate: The learning rate for gradient ascent\n",
    "        \"\"\"\n",
    "        # TODO: Exercise 1g - Implement a training loop over self.training_data that trains for num_iterations.\n",
    "        #  Every time, each sample has been seen, shuffle the  training data. Print your accuracy on the training\n",
    "        #  data and on the development data after each iteration. For evaluation you can reuse the function\n",
    "        #  self.evaluate.\n",
    "        \n",
    "        i=1\n",
    "        j=0\n",
    "        evaluate=0\n",
    "        train_accs=[]\n",
    "        dev_accs=[]\n",
    "        batches=[]\n",
    "        \n",
    "        while i<=num_iterations:\n",
    "            evaluate+=1\n",
    "            if evaluate == evaluate_after:\n",
    "                train_acc, dev_acc = self.evaluate()\n",
    "                train_accs.append(train_acc)\n",
    "                dev_accs.append(dev_acc)\n",
    "                batches.append(i)\n",
    "                evaluate = 0\n",
    "                print(f' After {i}/{num_iterations} iterations: ')\n",
    "                print(f' - Current train_acc is --> {train_acc}')\n",
    "                print(f' - Current dev_acc is -->  {dev_acc}')\n",
    "                print('===================')\n",
    "\n",
    "            if j == len(self.train_sentences):\n",
    "                random.shuffle(self.train_sentences)\n",
    "                j=0\n",
    "            \n",
    "            train_sentence= self.train_sentences[j]\n",
    "            new_theta = self.theta + learning_rate * (self.empirical_count - self.expected_feature_count(train_sentence))\n",
    "            self.theta=new_theta\n",
    "            i+=1\n",
    "            j+=1\n",
    "\n",
    "            # Create subplots with 1 row and 2 columns\n",
    "        fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "# Plot on the subplot\n",
    "        ax.plot(batches, train_accs, label='Train')\n",
    "        ax.plot(batches, dev_accs, label='Dev', color='orange')\n",
    "\n",
    "        # Set x and y labels\n",
    "        ax.set_xlabel('Iterations')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "\n",
    "        # Add a legend\n",
    "        ax.legend()\n",
    "\n",
    "        # Set the title\n",
    "        ax.set_title('Train vs Dev')\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Exercise 2 ###################################################################\n",
    "    def most_likely_label_sequence(self, sentence):\n",
    "        \"\"\"\n",
    "        :param sentence: A sentence we want to get a pos tag sequence for\n",
    "        :return: A list of predicted labels for the current sentence\n",
    "        \"\"\"\n",
    "        # TODO: Exercise 2 - Compute the most likely label sequence for a given sentence using the viterbi algorithm\n",
    "        #  We recommend using 2D arrays for gamma and delta\n",
    "        most_prob_seq=[]\n",
    "        delta=np.zeros((len(sentence), len(self.labels)))\n",
    "\n",
    "        for i, label in enumerate (self.labels):\n",
    "            delta[0,i] = self.psi(label, self.START_LABEL, sentence[0][0])\n",
    "        \n",
    "        gamma= get_key_from_value(self.label_indices, np.argmax(delta[0]))\n",
    "        most_prob_seq.append(gamma)\n",
    "\n",
    "\n",
    "        for w, (word,_) in enumerate(sentence[1:]):\n",
    "            for j, label in enumerate(self.labels):\n",
    "                current_max = 0\n",
    "                for i, prev_label in enumerate(self.labels):\n",
    "                    try:\n",
    "                        candidate_max = self.psi(label, prev_label, word)* delta[w,i] #  delta[w,i] is the previous delta with the previous word \n",
    "                    except RuntimeWarning:\n",
    "                        candidate_max= np.finfo(float).max\n",
    "                    \n",
    "                    if candidate_max > current_max:\n",
    "                        current_max = candidate_max\n",
    "                delta[w+1, j] = current_max\n",
    "            gamma =  get_key_from_value(self.label_indices, np.argmax(delta[w+1])) # retrieve the label of the max value for a given word. use w+1 since enuemrate starts from 0 \n",
    "            most_prob_seq.append(gamma)\n",
    "\n",
    "                    \n",
    "        return most_prob_seq\n",
    "\n",
    "    def evaluate(self, k=30):\n",
    "        train_sents = self.train_sentences\n",
    "        dev_sents = self.dev_sentences\n",
    "\n",
    "        # To reduce the workload of the evaluation method, we select a subset of sentences\n",
    "        if len(train_sents) > k:\n",
    "            train_sents = random.choices(self.train_sentences, k=k)\n",
    "        if len(dev_sents) > k:\n",
    "            dev_sents = random.choices(self.dev_sentences, k=k)\n",
    "\n",
    "        train_predictions = [list(zip(self.most_likely_label_sequence(s), [t[1] for t in s])) for s in\n",
    "                             train_sents]\n",
    "        dev_predictions = [list(zip(self.most_likely_label_sequence(s), [t[1] for t in s])) for s in\n",
    "                           dev_sents]\n",
    "\n",
    "        train_acc = sum([sum([1 if s[0] == s[1] else 0 for s in t]) / len(t) for t in train_predictions]) / len(\n",
    "            train_predictions)\n",
    "        dev_acc = sum([sum([1 if s[0] == s[1] else 0 for s in t]) / len(t) for t in dev_predictions]) / len(\n",
    "            dev_predictions)\n",
    "\n",
    "        return train_acc, dev_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=list('hello my name is vali'.split())\n",
    "x=np.zeros((len(s),10))\n",
    "x[len(s)-1, :]=1\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valif\\AppData\\Local\\Temp\\ipykernel_8824\\2254585781.py:130: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  value=self.psi(label, prev_label, token) * alpha[w,i]\n",
      "C:\\Users\\valif\\AppData\\Local\\Temp\\ipykernel_8824\\2254585781.py:130: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  value=self.psi(label, prev_label, token) * alpha[w,i]\n",
      "C:\\Users\\valif\\AppData\\Local\\Temp\\ipykernel_8824\\2254585781.py:171: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  beta[len(sentence)-1-(w+1),i]+= self.psi(label, prev_label, token) * beta[len(sentence)-1-w,j]\n",
      "C:\\Users\\valif\\AppData\\Local\\Temp\\ipykernel_8824\\2254585781.py:171: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  beta[len(sentence)-1-(w+1),i]+= self.psi(label, prev_label, token) * beta[len(sentence)-1-w,j]\n",
      "C:\\Users\\valif\\AppData\\Local\\Temp\\ipykernel_8824\\2254585781.py:171: RuntimeWarning: invalid value encountered in scalar add\n",
      "  beta[len(sentence)-1-(w+1),i]+= self.psi(label, prev_label, token) * beta[len(sentence)-1-w,j]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " After 20/500 iterations: \n",
      " - Current train_acc is --> 0.0022222222222222222\n",
      " - Current dev_acc is -->  0.0031851851851851854\n",
      "===================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m corpus\u001b[38;5;241m=\u001b[39mimport_corpus(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorpus_pos.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m=\u001b[39mLinearChainCRF(corpus)\n\u001b[1;32m----> 3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m500\u001b[39m, evaluate_after\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "Cell \u001b[1;32mIn [6], line 271\u001b[0m, in \u001b[0;36mLinearChainCRF.train\u001b[1;34m(self, num_iterations, learning_rate, evaluate_after)\u001b[0m\n\u001b[0;32m    269\u001b[0m evaluate\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate \u001b[38;5;241m==\u001b[39m evaluate_after:\n\u001b[1;32m--> 271\u001b[0m     train_acc, dev_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m     train_accs\u001b[38;5;241m.\u001b[39mappend(train_acc)\n\u001b[0;32m    273\u001b[0m     dev_accs\u001b[38;5;241m.\u001b[39mappend(dev_acc)\n",
      "Cell \u001b[1;32mIn [6], line 361\u001b[0m, in \u001b[0;36mLinearChainCRF.evaluate\u001b[1;34m(self, k)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dev_sents) \u001b[38;5;241m>\u001b[39m k:\n\u001b[0;32m    359\u001b[0m     dev_sents \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoices(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdev_sentences, k\u001b[38;5;241m=\u001b[39mk)\n\u001b[1;32m--> 361\u001b[0m train_predictions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmost_likely_label_sequence(s), [t[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m s])) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m    362\u001b[0m                      train_sents]\n\u001b[0;32m    363\u001b[0m dev_predictions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmost_likely_label_sequence(s), [t[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m s])) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m    364\u001b[0m                    dev_sents]\n\u001b[0;32m    366\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28msum\u001b[39m([\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m s[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m s[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m t]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m train_predictions]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m    367\u001b[0m     train_predictions)\n",
      "Cell \u001b[1;32mIn [6], line 361\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dev_sents) \u001b[38;5;241m>\u001b[39m k:\n\u001b[0;32m    359\u001b[0m     dev_sents \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoices(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdev_sentences, k\u001b[38;5;241m=\u001b[39mk)\n\u001b[1;32m--> 361\u001b[0m train_predictions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_likely_label_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m, [t[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m s])) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m    362\u001b[0m                      train_sents]\n\u001b[0;32m    363\u001b[0m dev_predictions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmost_likely_label_sequence(s), [t[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m s])) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m    364\u001b[0m                    dev_sents]\n\u001b[0;32m    366\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28msum\u001b[39m([\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m s[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m s[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m t]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m train_predictions]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m    367\u001b[0m     train_predictions)\n",
      "Cell \u001b[1;32mIn [6], line 342\u001b[0m, in \u001b[0;36mLinearChainCRF.most_likely_label_sequence\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m:\n\u001b[0;32m    340\u001b[0m         candidate_max\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfinfo(\u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m.\u001b[39mmax\n\u001b[1;32m--> 342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcandidate_max\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcurrent_max\u001b[49m:\n\u001b[0;32m    343\u001b[0m         current_max \u001b[38;5;241m=\u001b[39m candidate_max\n\u001b[0;32m    344\u001b[0m delta[w\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, j] \u001b[38;5;241m=\u001b[39m current_max\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corpus=import_corpus('corpus_pos.txt')\n",
    "model=LinearChainCRF(corpus)\n",
    "model.train(500, evaluate_after=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
