{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical NLP - Assignment 01\n",
    "\n",
    "Group members: Ilaria Salvatori, Vali Florinel Craciun "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Download the file corpus.zip from LernraumPlus. The file corpus.txt contains the corpus we will use in this\n",
    "worksheet, there is exactly one sentence in each line of this file. A sentence is a sequence $w_1, . . . , w_N$ of words, where\n",
    "$w_1$ is the first word in the sentence, $w_N$ is the last word and $N$ is the number of words in the sentence. Now we define\n",
    "some distributions of words for the provided corpus:\n",
    "- $P (w)$ is the distribution of all words in the corpus\n",
    "- $P (w_i|w_{i−1})$ is the distribution of words given the previous word in a word sequence is $w_{i−1}$\n",
    "- $P (w_i|w_{i−1}, w_{i−2})$ is the distribution of words at position i in a word sequence given the word at position $i-1$ is\n",
    "$w_{i−1}$ and the word at position $i −2$ is $w_{i−2}$\n",
    "\n",
    "**Hint** Introduce special words to model the beginning and the end of a sentence!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) First of all you need to tokenize the corpus. Therefore, implement a Python function `tokenize_sentence()`\n",
    "which takes a single string as input (representing a sentence) and returns a list of words. You may ignore commas,\n",
    "semicolons and colons. (*3 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(doc):\n",
    "    words=[]\n",
    "    to_ignore=[',', ':', ';' ] # chars to ignore\n",
    "    \n",
    "\n",
    "    with open(doc,'r') as f:\n",
    "        for line in f: \n",
    "            for word in line.split(): # each line is splitted \n",
    "                if word not in to_ignore:\n",
    "                    \n",
    "                    if word[-1]=='.' :  # we want to separate the words with a final point \n",
    "                        wordPre=word[:-1]\n",
    "                        \n",
    "                        if len(wordPre)>1: # avoid index out of range error in case word == '.'\n",
    "                        \n",
    "                            if wordPre[-1]=='.' :  # we also check for duplicate punctuation \n",
    "                                wordPre = wordPre[:-1]\n",
    "                        \n",
    "                        wordPost=word[-1] # get the last char of the current word \n",
    "                        # append both parts \n",
    "                        words.append(wordPre)\n",
    "                        words.append(wordPost)\n",
    "                    \n",
    "                    elif word[-1]==',':\n",
    "                        wordPre=word[0:-1]\n",
    "                        if wordPre[-1]==',':\n",
    "                            wordPre = wordPre[0:-1]\n",
    "\n",
    "                        words.append(wordPre)\n",
    "                    \n",
    "                    elif word[-1]==';':\n",
    "                        wordPre=word[0:-1]\n",
    "                        if wordPre[-1]==';':\n",
    "                            wordPre = wordPre[0:-1]\n",
    "                        words.append(wordPre)\n",
    "\n",
    "                    elif word[-1]==':':\n",
    "                        wordPre=word[0:-1]\n",
    "                        if wordPre[-1]==':':\n",
    "                            wordPre = wordPre[0:-1]\n",
    "                        words.append(wordPre)\n",
    "                    else:\n",
    "                        words.append(word)\n",
    "\n",
    "    return words \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words= tokenize_sentence('corpus.txt') # the output is a list with all words \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Provide Python code for representing and learning the distributions $P (w)$, $P (w_i|w_{i−1})$ and $P (w_i|w_{i−1}, w_{i−2})$.\n",
    "To do this, implement the following functions which should return a probability distribution over the whole\n",
    "vocabulary:\n",
    "-  `unigram_distribution()`\n",
    "- `bigram_distribution(w1)`\n",
    "- `trigram_distribution(w1, w2)`\n",
    "\n",
    "(*6 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unigram_distribution(words):\n",
    "    total_words = len(words)\n",
    "    probs = {}\n",
    "    \n",
    "    # for each word we get the probability by adding to the dictionary 1/total_words every time we encounter the word\n",
    "    for word in words:\n",
    "        if word in probs:\n",
    "            probs[word] += 1 / total_words\n",
    "        else:\n",
    "            probs[word] = 1 / total_words\n",
    "    \n",
    "    return probs\n",
    "\n",
    "\n",
    "def bigram_distribution(w1, words,probs):\n",
    "    # we consider w1 as wi-1 of the slides \n",
    "    cond_probs={}\n",
    "    indices = [i for i, x in enumerate(words) if x == w1] # indices of w1\n",
    "    co_occ = [(words[i],words[i+1]) for i in indices] # co occurence list. This is a list with all the tuples (wi-1,wi)\n",
    "    \n",
    "    for el in co_occ:\n",
    "        w1_freq = probs[el[0]]*len(words) # frequency of wi-1\n",
    "        \n",
    "        #same process as before but now we want f(wi,wi-1)/f(wi-1)\n",
    "        # so every time we observe a certain (wi-1,wi) we add 1/f(wi-1)\n",
    "        if el in cond_probs:\n",
    "            cond_probs[el] += 1 / w1_freq \n",
    "        else:\n",
    "            cond_probs[el] = 1 / w1_freq\n",
    "\n",
    "\n",
    "    return cond_probs \n",
    "\n",
    "\n",
    "def trigram_distribution(w1,w2, words, probs):   \n",
    "    # w2 is wi-2\n",
    "    \n",
    "    probs=unigram_distribution(words)\n",
    "    cond_probs={}\n",
    "    \n",
    "    indices_w2 = [i for i, x in enumerate(words) if x == w2] # get the indices of wi-2 \n",
    "    co_occ=[(words[i], words[i+1], words[i+2]) for i in indices_w2 if words[i+1] == w1] # get the triple (wi-2, wi-1, wi) iff the word after w2 is the specified w1\n",
    "    for el in co_occ:\n",
    "        p=bigram_distribution(w2,words, probs)\n",
    "        w1_w2_freq = p[(w2,w1)]*probs[w2]*len(words) #since bigram returns conditional probabilities \n",
    "                                                     #we need to multiply them by the p(w2) from the unigram, and then multiply everything by len (words) to get f(w2,w1)\n",
    "        \n",
    "        # do the same probability update as before\n",
    "        if el in cond_probs:\n",
    "            cond_probs[el] += 1 / w1_w2_freq\n",
    "        else:\n",
    "            cond_probs[el] = 1 / w1_w2_freq\n",
    "\n",
    "\n",
    "    return cond_probs\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) How does the number of parameters of these distributions scale with the number of different words in the corpus?\n",
    "Explain your answer! \n",
    "(*1 point*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the unigram distribution the addition of new words leads to the additon of new probabilities for each of these words. This happens because the probability of each word is intependent from the other words.\\\n",
    "In the case of bigram and trigram the addition of new words may or may not introduce new combination. In particular, if the new words are conjuncted to already existing words, new probabilities are added to the distribution. On the contrary, if these new words appear in isolation, we won't add any probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Implement a function `sample(distribution)` for drawing a sample from the distributions $P (w)$, $P (w_i|w_{i−1})$\n",
    "and $P (w_i|w_{i−1}, w_{i−2})$ according to the algorithm presented in the lecture. Make use of your solution of Task 1.\\\n",
    "(*3 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs=unigram_distribution(words) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(distribution, words, w1=None , w2=None , probs=None):\n",
    "   # choose the distribution from which to sample \n",
    "    if distribution == 'unigram':\n",
    "        p=unigram_distribution(words)\n",
    "    \n",
    "    elif distribution == 'bigram':\n",
    "        p=bigram_distribution(w1, words, probs)\n",
    "    else:\n",
    "        p=trigram_distribution(w1, w2, words, probs)\n",
    "\n",
    "    # dictionaries are unordered but this is a little trick to sort them by probabilities in reverse order\n",
    "    sorted_p = dict(sorted(p.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    x=random.random()\n",
    "    values= list(sorted_p.values()) # sorted probabilities  \n",
    "\n",
    "    sump = values[0] # cumulative sum starting with the highest probability \n",
    "\n",
    "    # with this for loop we are summing one probability at a time and cheking if the sum exceeds x.\n",
    "    for i in range(0,len(values)) :\n",
    "\n",
    "        if sump - x < 0:\n",
    "            sump+=values[i+1]\n",
    "        else: \n",
    "            break \n",
    "\n",
    "    # when the break condition happens we check for the corresponding key. It may happen that more keys has the same probability\n",
    "    # in this case we return the first one since we don't care about if one or the other if first\n",
    "    for key, value in sorted_p.items():\n",
    "        if value == values[i]:\n",
    "            break\n",
    "\n",
    "    return key \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Use the statistical information of the provided corpus to implement three different sentence generators, i.e., use\n",
    "the distributions $P (w)$, $P (w_i|w_{i−1})$, $P (w_i|w_{i−1}, w_{i−2})$ and your code of a) to successively generate single words.\n",
    "In other words, your first sentence generator should use the distribution $P (w)$, the second one $P (w_i|w_{i−1}) $ and the\n",
    "third one $P (w_i|w_{i−1}, w_{i−2})$. The sentence should be returned as a string. Use the following naming conventions:\n",
    "- `generate_sentence_unigram()`\n",
    "- `generate_sentence_bigram()`\n",
    "- `generate_sentence_trigram()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_unigram(words):\n",
    "    # this code exactly mimics the pseudo code of the slides \n",
    "    eos=False\n",
    "    sentence='' # initialize the sentence with an empty string \n",
    "    while not eos:\n",
    "        w=sample('unigram', words) # sample from unigram \n",
    "        sentence+=w+ ' ' # add the word \n",
    "        if w=='.': # stop if a dot is sampled \n",
    "            eos=True\n",
    "    \n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"identity is as the his is been short was traveling and on of an mort front satisfactory familiar -- spite branch on program they ghosts or produced bones the lindsey's time stop many of of them sharp of were the mere could they house a that as minneapolis . \""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence_unigram(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_bigram(words, probs):\n",
    "    \n",
    "    eos=False\n",
    "    w1 = sample('unigram', words)# get the starting word froma  unigram\n",
    "    if w1=='.': # check the same stopping conditiona as in the sentence generated by the unigram\n",
    "        eos=True\n",
    "\n",
    "    while not eos:\n",
    "        w=sample('bigram', words, w1, probs= probs )[-1] # get the last word in the tuple \n",
    "        sentence+= ' ' + w # add the word \n",
    "        if w=='.': # check the conditiona again \n",
    "            eos=True\n",
    "        w1=w # the current word becomes the prevous one \n",
    "    \n",
    "    return sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'was not already given to the world and different scanning techniques may be quickly as a central wrangled among other test predispositions destructive of the winter sky was obvious that only by lot of an important because of the peasants still every first contact a man to his work the af curve is always had a good thing to feel the wall street .'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence_bigram(words, probs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_trigram(words, probs):\n",
    "\n",
    "    eos=False\n",
    "    # sample w2 and w1 \n",
    "    w2=sample('unigram', words) \n",
    "    w1=sample('bigram', words, w1=w2, probs=probs)[-1] # sample w1 using w2 as previous word and get only the last elemnt of the tuple\n",
    "\n",
    "    if w2=='.': # same stopping condition if the first word is a dot \n",
    "        eos=True\n",
    "        \n",
    "    sentence=w2+' '+ w1+' ' # concatenate the two words  \n",
    "    if w1=='.':\n",
    "        eos=True\n",
    "\n",
    "    while not eos:\n",
    "        wi=sample('trigram',words, probs=probs, w1=w1, w2=w2)[-1] # sample from the trigram and get the last word of the triple \n",
    "        \n",
    "        sentence+=wi+' ' # add the new word \n",
    "        \n",
    "        if wi=='.': # check the condition again \n",
    "            eos=True\n",
    "\n",
    "        w2=w1 # wi-2 <-- wi-1\n",
    "        w1=wi # wi-1 <-- wi \n",
    "        # and look for the new wi\n",
    "\n",
    "\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in their first three games the longhorns have had the ball and handed it back . '"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence_trigram(words,probs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Describe the results of the three sentence generators you implemented. Try to explain the results.\\\n",
    "(*1 point*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more we shift towards trigram generated phrases from the unigram ones, we can observe the following:\n",
    "- Phrases are shorter but more meaningful. \n",
    "- The algorithm becomes even more sensitive to initial choice of words, leading to a time increse of sentence generation (i.e. bad starting point can lead to very high waiting time).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
