{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(corpus): # to get the list of distinct labels\n",
    "    labels=set()\n",
    "    \n",
    "    \n",
    "    for sentence in corpus:\n",
    "        for word in sentence:\n",
    "            labels.add(word[1])\n",
    "    \n",
    "   \n",
    "    \n",
    "    return labels \n",
    "\n",
    "\n",
    "def get_tokens(corpus): # to get the list of distinct tokens (i.e. words)\n",
    "\n",
    "    tokens=set()\n",
    "    for sentence in corpus:\n",
    "        for word in sentence:\n",
    "            tokens.add(word[0])\n",
    "            \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_corpus(path_to_file):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    \n",
    "    \n",
    "    with open(path_to_file) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if len(line) == 0:\n",
    "                sentences.append(sentence)    \n",
    "                sentence = []\n",
    "                continue\n",
    "                    \n",
    "            pair = line.split(' ')\n",
    "            sentence.append((pair[0], pair[-1]))\n",
    "            \n",
    "        if len(sentence) > 0:\n",
    "            sentences.append(sentence)\n",
    "                \n",
    "    return sentences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MaxEntModel(object):\n",
    "    # training corpus\n",
    "    def __init__(self, path) :\n",
    "    \n",
    "        self.corpus = import_corpus(path)\n",
    "        \n",
    "        # (numpy) array containing the parameters of the model\n",
    "        # has to be initialized by the method 'initialize'\n",
    "        self.theta = None\n",
    "        \n",
    "        # dictionary containing all possible features of a corpus and their corresponding index;\n",
    "        # has to be set by the method 'initialize'; hint: use a Python dictionary\n",
    "        self.feature_indices = None\n",
    "        \n",
    "        # set containing a list of possible lables\n",
    "        # has to be set by the method 'initialize'\n",
    "        self.labels = None\n",
    "        \n",
    "        self.tokens=None \n",
    "\n",
    "        self.train_size=0\n",
    "\n",
    "        self.words_used=[]\n",
    "\n",
    "    def initialize(self, train_test_split=0.9):\n",
    "        '''\n",
    "        Initialize the maximun entropy model, i.e., build the set of all features, the set of all labels\n",
    "        and create an initial array 'theta' for the parameters of the model.\n",
    "        Parameters: corpus: list of list representing the corpus, returned by the function 'import_corpus'\n",
    "        '''\n",
    "        features={}\n",
    "        # create train/test split\n",
    "        num_train_sentences = round(len(self.corpus) * train_test_split)\n",
    "        self.train_sentences = self.corpus[:num_train_sentences]\n",
    "        self.test_sentences = self.corpus[num_train_sentences:]\n",
    "        self.tokens=get_tokens(self.corpus)\n",
    "        self.labels=get_labels(self.corpus)\n",
    "\n",
    "        i=0\n",
    "        \n",
    "        for token in self.tokens:\n",
    "            for label in self.labels:\n",
    "                features[(f'{token}'),(f'{label}')]=i\n",
    "                i+=1\n",
    "\n",
    "        for label1 in self.labels:\n",
    "            for label2 in self.labels:\n",
    "                features[(f'{label1}'),(f'{label2}')]=i\n",
    "                i+=1\n",
    "\n",
    "        for label in self.labels:\n",
    "            features[('START'),(f'{label}')]=i\n",
    "            i+=1\n",
    "\n",
    "        self.feature_indices=features\n",
    "        self.theta=np.zeros(len(self.feature_indices)) +1\n",
    "\n",
    "        for sentence in self.train_sentences:\n",
    "            self.train_size+=len(sentence)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_active_features(self, word, label, prev_label):\n",
    "        '''\n",
    "        Compute the vector of active features.\n",
    "        Parameters: word: string; a word at some position i of a given sentence\n",
    "                    label: string; a label assigned to the given word\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: (numpy) array containing only zeros and ones.\n",
    "        '''\n",
    "        \n",
    "        # your code here\n",
    "        active_features= np.zeros(len(self.feature_indices))\n",
    "\n",
    "\n",
    "        for sentence in self.train_sentences:\n",
    "            for i, (word_i,label_i) in enumerate(sentence):\n",
    "                prev_label_i= 'START' if i==0 else sentence[i-1][1]\n",
    "                \n",
    "                if (word_i==word and label_i==label):\n",
    "                    index=self.feature_indices[(word_i),(label_i)]\n",
    "                    active_features[index]=1\n",
    "\n",
    "                if (label_i==label and prev_label_i== prev_label):\n",
    "                    index=self.feature_indices[(prev_label_i),(label_i)]\n",
    "                    active_features[index]=1\n",
    "\n",
    "        return active_features\n",
    "\n",
    "\n",
    "    def cond_normalization_factor(self, word, prev_label):\n",
    "        '''\n",
    "        Compute the normalization factor 1/Z(x_i).\n",
    "        Parameters: word: string; a word x_i at some position i of a given sentence\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: float\n",
    "        '''\n",
    "        z=0.0\n",
    "\n",
    "        for label in self.labels:\n",
    "            z += np.dot(self.theta,self.get_active_features(word, label, prev_label))\n",
    "\n",
    "        return 1/z\n",
    "     \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Exercise 2 b) ###################################################################\n",
    "    def conditional_probability(self, word,label, prev_label):\n",
    "        '''\n",
    "        Compute the conditional probability of a label given a word x_i.\n",
    "        Parameters: label: string; we are interested in the conditional probability of this label\n",
    "                    word: string; a word x_i some position i of a given sentence\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: float\n",
    "        '''\n",
    "        return np.exp(np.dot(self.theta, self.get_active_features(word,label, prev_label)))*self.cond_normalization_factor(word, prev_label)\n",
    "         \n",
    "    \n",
    "    \n",
    "    def empirical_feature_count(self, word, label, prev_label):\n",
    "        '''\n",
    "        Compute the empirical feature count given a word, the actual label of this word and the label of the previous word.\n",
    "        Parameters: word: string; a word x_i some position i of a given sentence\n",
    "                    label: string; the actual label of the given word\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: (numpy) array containing the empirical feature count\n",
    "        '''\n",
    "        \n",
    "        emp_feat_count=np.zeros(len(self.feature_indices) ) \n",
    "        count1=0 # this is for w,l\n",
    "        count2=0 # this is for pl,l\n",
    "\n",
    "        for sentence in self.train_sentences:\n",
    "            \n",
    "            for i, (word_i, label_i) in enumerate (sentence):\n",
    "                prev_label_i= 'START' if i==0 else sentence[i-1][1]\n",
    "\n",
    "                if word_i==word and label_i==label:\n",
    "                    count1+=1\n",
    "                \n",
    "                if label_i==label and prev_label_i==prev_label:\n",
    "                    count2+=1\n",
    "\n",
    "            index1=self.feature_indices[(word),(label)]\n",
    "            emp_feat_count[index1]=count1\n",
    "\n",
    "            index2=self.feature_indices[(prev_label),(label)]\n",
    "            emp_feat_count[index2]=count2\n",
    "\n",
    "\n",
    "        return emp_feat_count/self.train_size\n",
    "\n",
    "            \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # Exercise 3 b) ###################################################################\n",
    "    def expected_feature_count(self, word, prev_label):\n",
    "        '''\n",
    "        Compute the expected feature count given a word, the label of the previous word and the parameters of the current model\n",
    "        (see variable theta)\n",
    "        Parameters: word: string; a word x_i some position i of a given sentence\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: (numpy) array containing the expected feature count\n",
    "        '''\n",
    "    \n",
    "        exp_feat_count= np.zeros(len(self.feature_indices)) # final vector\n",
    "\n",
    "        for label in self.labels:\n",
    "            cond_p=self.conditional_probability(word, label, prev_label)\n",
    "            af= self.get_active_features(word, label, prev_label) # current active feats\n",
    "      \n",
    "            exp_feat_count+= (cond_p*af) # update for all labels \n",
    "\n",
    "        return exp_feat_count/self.train_size\n",
    "    \n",
    "\n",
    "    def parameter_update(self, word, label, prev_label, learning_rate=0.1):\n",
    "        '''\n",
    "        Do one learning step.\n",
    "        Parameters: word: string; a randomly selected word x_i at some position i of a given sentence\n",
    "                    label: string; the actual label of the selected word\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "                    learning_rate: float\n",
    "        '''\n",
    "        \n",
    "        new_theta=self.theta+learning_rate*(self.empirical_feature_count(word,label,prev_label)\n",
    "                                            - self.expected_feature_count(word, prev_label))\n",
    "    \n",
    "        self.theta=new_theta\n",
    "\n",
    "    def train(self, number_iterations, learning_rate=0.1):\n",
    "        '''\n",
    "        Implement the training procedure.\n",
    "        Parameters: number_iterations: int; number of parameter updates to do\n",
    "                    learning_rate: float\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        for _ in range (number_iterations):\n",
    "            sentence=random.choice(self.train_sentences)\n",
    "            \n",
    "            i,(word,label)=random.choice(list(enumerate(sentence)))\n",
    "            prev_label='START' if i==0 else sentence[i-1][1] \n",
    "            self.words_used.append(i+1)\n",
    "            self.parameter_update(word, label, prev_label)\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Exercise 4 c) ###################################################################\n",
    "    def predict(self, word, prev_label):\n",
    "        '''\n",
    "        Predict the most probable label of the word referenced by 'word'\n",
    "        Parameters: word: string; a word x_i at some position i of a given sentence\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: string; most probable label\n",
    "        '''\n",
    "        preds=[]\n",
    "        \n",
    "        for label in self.labels:\n",
    "            prob=self.conditional_probability(word, label, prev_label)\n",
    "            preds.append((label, prob))\n",
    "\n",
    "        preds.sort(reverse=True)\n",
    "\n",
    "\n",
    "        return preds[0] \n",
    "\n",
    "    \n",
    "    def empirical_feature_count_batch(self, sentences):\n",
    "        '''\n",
    "        Predict the empirical feature count for a set of sentences\n",
    "        Parameters: sentences: list; a list of sentences; should be a sublist of the list returnd by 'import_corpus'\n",
    "        Returns: (numpy) array containing the empirical feature count\n",
    "        '''\n",
    "        \n",
    "        emp_feat_count=np.zeros(len(self.feature_indices))\n",
    "\n",
    "        for sentence in sentences:\n",
    "            if self.iter==0:\n",
    "                self.words_used.append(len(sentence))\n",
    "            else:\n",
    "                self.words_used.append(self.words_used[-1] + len(sentence))  \n",
    "            \n",
    "            for i, (word,label) in enumerate(sentence):\n",
    "                prev_label='START' if i==0 else sentence[i-1][1]\n",
    "                emp_feat_count+=self.empirical_feature_count(word, label, prev_label)\n",
    "        \n",
    "        return emp_feat_count/self.train_size\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Exercise 5 a) ###################################################################\n",
    "    def expected_feature_count_batch(self, sentences):\n",
    "        '''\n",
    "        Predict the expected feature count for a set of sentences\n",
    "        Parameters: sentences: list; a list of sentences; should be a sublist of the list returnd by 'import_corpus'\n",
    "        Returns: (numpy) array containing the expected feature count\n",
    "        '''\n",
    "        \n",
    "        exp_feat_count=np.zeros(len(self.feature_indices))\n",
    "\n",
    "        for sentence in sentences:\n",
    "            for i, (word,label) in enumerate(sentence):\n",
    "                prev_label='START' if i==0 else sentence[i-1][1]\n",
    "                exp_feat_count+=self.expected_feature_count(word, prev_label)\n",
    "\n",
    "        return exp_feat_count/self.train_size\n",
    "    \n",
    "    \n",
    "    def train_batch(self, number_iterations, batch_size, learning_rate=0.1):\n",
    "        '''\n",
    "        Implement the training procedure which uses 'batch_size' sentences from to training corpus\n",
    "        to compute the gradient.\n",
    "        Parameters: number_iterations: int; number of parameter updates to do\n",
    "                    batch_size: int; number of sentences to use in each iteration\n",
    "                    learning_rate: float\n",
    "        '''\n",
    "        \n",
    "        sentences=random.choices(self.train_sentences, k=batch_size)\n",
    "\n",
    "        for i in range(number_iterations):\n",
    "            self.iter=i\n",
    "            new_theta=self.theta+ learning_rate*(self.empirical_feature_count_batch(sentences)\n",
    "                                                 -self.expected_feature_count_batch(sentences))\n",
    "            \n",
    "\n",
    "        self.theta=new_theta\n",
    "\n",
    "\n",
    "    def accuracy(self, test):\n",
    "\n",
    "        correct=0\n",
    "        total=0\n",
    "\n",
    "        for sentence in test:\n",
    "\n",
    "            for i, (word,true) in enumerate(sentence):\n",
    "                prev_label = \"START\" if i == 0 else sentence[i - 1][1]\n",
    "                pred=self.predict(word, prev_label)\n",
    "                total+=1\n",
    "                if pred==true:\n",
    "                    correct+=1\n",
    "\n",
    "        return correct/total\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def evaluate(self, test, num_iter, model_type):\n",
    "\n",
    "        accuracies=[]\n",
    "        \n",
    "        \n",
    "        if model_type == 'Normal':\n",
    "                # Train using train method\n",
    "                self.train(1)\n",
    "                accuracies.append(self.accuracy(test))\n",
    "        \n",
    "        elif model_type == 'Batch':\n",
    "                # Train using train_batch method with one sentence at a time\n",
    "            self.train_batch(1, 1)\n",
    "            accuracies.append(self.accuracy(test))\n",
    "\n",
    "\n",
    "        plt.plot(self.words_used, accuracies)\n",
    "        plt.xlabel('Accuracy')\n",
    "        plt.ylabel('N of words used')\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train=MaxEntModel('corpus_pos.txt')\n",
    "model_train.initialize()\n",
    "\n",
    "model_batch=MaxEntModel('corpus_pos.txt')\n",
    "model_batch.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [62], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_train\u001b[38;5;241m.\u001b[39mevaluate(model_train\u001b[38;5;241m.\u001b[39mtest_sentences, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNormal\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn [60], line 352\u001b[0m, in \u001b[0;36mMaxEntModel.evaluate\u001b[1;34m(self, test, num_iter, model_type)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNormal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    350\u001b[0m         \u001b[38;5;66;03m# Train using train method\u001b[39;00m\n\u001b[0;32m    351\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 352\u001b[0m         accuracies\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatch\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    355\u001b[0m         \u001b[38;5;66;03m# Train using train_batch method with one sentence at a time\u001b[39;00m\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_batch(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn [60], line 334\u001b[0m, in \u001b[0;36mMaxEntModel.accuracy\u001b[1;34m(self, test)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (word,true) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sentence):\n\u001b[0;32m    333\u001b[0m     prev_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTART\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m sentence[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 334\u001b[0m     pred\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m     total\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pred\u001b[38;5;241m==\u001b[39mtrue:\n",
      "Cell \u001b[1;32mIn [60], line 254\u001b[0m, in \u001b[0;36mMaxEntModel.predict\u001b[1;34m(self, word, prev_label)\u001b[0m\n\u001b[0;32m    251\u001b[0m preds\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels:\n\u001b[1;32m--> 254\u001b[0m     prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconditional_probability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    255\u001b[0m     preds\u001b[38;5;241m.\u001b[39mappend((label, prob))\n\u001b[0;32m    257\u001b[0m preds\u001b[38;5;241m.\u001b[39msort(reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn [60], line 145\u001b[0m, in \u001b[0;36mMaxEntModel.conditional_probability\u001b[1;34m(self, word, label, prev_label)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconditional_probability\u001b[39m(\u001b[38;5;28mself\u001b[39m, word,label, prev_label):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;124;03m    Compute the conditional probability of a label given a word x_i.\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    Parameters: label: string; we are interested in the conditional probability of this label\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m    Returns: float\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mexp(np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_active_features(word,label, prev_label)))\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcond_normalization_factor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_label\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [60], line 127\u001b[0m, in \u001b[0;36mMaxEntModel.cond_normalization_factor\u001b[1;34m(self, word, prev_label)\u001b[0m\n\u001b[0;32m    124\u001b[0m z\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels:\n\u001b[1;32m--> 127\u001b[0m     z \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_active_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_label\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mz\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_train.evaluate(model_train.test_sentences, 100, 'Normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_batch.evaluate(model_train.test_sentences, 100, 'Batch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
