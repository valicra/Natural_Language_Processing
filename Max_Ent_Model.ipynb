{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(corpus): # to get the list of distinct labels\n",
    "    labels=set()\n",
    "    \n",
    "    \n",
    "    for sentence in corpus:\n",
    "        for word in sentence:\n",
    "            labels.add(word[1])\n",
    "    \n",
    "   \n",
    "    \n",
    "    return labels \n",
    "\n",
    "\n",
    "def get_tokens(corpus): # to get the list of distinct tokens (i.e. words)\n",
    "\n",
    "    tokens=set()\n",
    "    for sentence in corpus:\n",
    "        for word in sentence:\n",
    "            tokens.add(word[0])\n",
    "            \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_corpus(path_to_file):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    \n",
    "    \n",
    "    with open(path_to_file) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if len(line) == 0:\n",
    "                sentences.append(sentence)    \n",
    "                sentence = []\n",
    "                continue\n",
    "                    \n",
    "            pair = line.split(' ')\n",
    "            sentence.append((pair[0], pair[-1]))\n",
    "            \n",
    "        if len(sentence) > 0:\n",
    "            sentences.append(sentence)\n",
    "                \n",
    "    return sentences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MaxEntModel(object):\n",
    "    # training corpus\n",
    "    def __init__(self, path) :\n",
    "    \n",
    "        self.corpus = import_corpus(path)\n",
    "        \n",
    "        # (numpy) array containing the parameters of the model\n",
    "        # has to be initialized by the method 'initialize'\n",
    "        self.theta = None\n",
    "        \n",
    "        # dictionary containing all possible features of a corpus and their corresponding index;\n",
    "        # has to be set by the method 'initialize'; hint: use a Python dictionary\n",
    "        self.feature_indices = None\n",
    "        \n",
    "        # set containing a list of possible lables\n",
    "        # has to be set by the method 'initialize'\n",
    "        self.labels = None\n",
    "        \n",
    "        self.tokens=None \n",
    "\n",
    "        self.train_size=0\n",
    "\n",
    "        self.words_used=[]\n",
    "\n",
    "    def initialize(self, train_test_split=0.9):\n",
    "        '''\n",
    "        Initialize the maximun entropy model, i.e., build the set of all features, the set of all labels\n",
    "        and create an initial array 'theta' for the parameters of the model.\n",
    "        Parameters: corpus: list of list representing the corpus, returned by the function 'import_corpus'\n",
    "        '''\n",
    "        features={}\n",
    "        # create train/test split\n",
    "        num_train_sentences = round(len(self.corpus) * train_test_split)\n",
    "        self.train_sentences = self.corpus[:num_train_sentences]\n",
    "        self.test_sentences = self.corpus[num_train_sentences:]\n",
    "        self.tokens=get_tokens(self.corpus)\n",
    "        self.labels=get_labels(self.corpus)\n",
    "\n",
    "        i=0\n",
    "        \n",
    "        for token in self.tokens:\n",
    "            for label in self.labels:\n",
    "                features[(f'{token}'),(f'{label}')]=i\n",
    "                i+=1\n",
    "\n",
    "        for label1 in self.labels:\n",
    "            for label2 in self.labels:\n",
    "                features[(f'{label1}'),(f'{label2}')]=i\n",
    "                i+=1\n",
    "\n",
    "        for label in self.labels:\n",
    "            features[('START'),(f'{label}')]=i\n",
    "            i+=1\n",
    "\n",
    "        self.feature_indices=features\n",
    "        self.theta=np.zeros(len(self.feature_indices)) +1\n",
    "\n",
    "        for sentence in self.train_sentences:\n",
    "            self.train_size+=len(sentence)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_active_features(self, word, label, prev_label):\n",
    "        '''\n",
    "        Compute the vector of active features.\n",
    "        Parameters: word: string; a word at some position i of a given sentence\n",
    "                    label: string; a label assigned to the given word\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: (numpy) array containing only zeros and ones.\n",
    "        '''\n",
    "        \n",
    "        # your code here\n",
    "        active_features= np.zeros(len(self.feature_indices))\n",
    "\n",
    "\n",
    "        for sentence in self.train_sentences:\n",
    "            for i, (word_i,label_i) in enumerate(sentence):\n",
    "                prev_label_i= 'START' if i==0 else sentence[i-1][1]\n",
    "                \n",
    "                if (word_i==word and label_i==label):\n",
    "                    index=self.feature_indices[(word_i),(label_i)]\n",
    "                    active_features[index]=1\n",
    "\n",
    "                if (label_i==label and prev_label_i== prev_label):\n",
    "                    index=self.feature_indices[(prev_label_i),(label_i)]\n",
    "                    active_features[index]=1\n",
    "\n",
    "        return active_features\n",
    "\n",
    "\n",
    "    def cond_normalization_factor(self, word, prev_label):\n",
    "        '''\n",
    "        Compute the normalization factor 1/Z(x_i).\n",
    "        Parameters: word: string; a word x_i at some position i of a given sentence\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: float\n",
    "        '''\n",
    "        z=0.0\n",
    "\n",
    "        for label in self.labels:\n",
    "            z += np.dot(self.theta,self.get_active_features(word, label, prev_label))\n",
    "\n",
    "        return 1/z\n",
    "     \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Exercise 2 b) ###################################################################\n",
    "    def conditional_probability(self, word,label, prev_label):\n",
    "        '''\n",
    "        Compute the conditional probability of a label given a word x_i.\n",
    "        Parameters: label: string; we are interested in the conditional probability of this label\n",
    "                    word: string; a word x_i some position i of a given sentence\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: float\n",
    "        '''\n",
    "        return np.exp(np.dot(self.theta, self.get_active_features(word,label, prev_label)))*self.cond_normalization_factor(word, prev_label)\n",
    "         \n",
    "    \n",
    "    \n",
    "    def empirical_feature_count(self, word, label, prev_label):\n",
    "        '''\n",
    "        Compute the empirical feature count given a word, the actual label of this word and the label of the previous word.\n",
    "        Parameters: word: string; a word x_i some position i of a given sentence\n",
    "                    label: string; the actual label of the given word\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: (numpy) array containing the empirical feature count\n",
    "        '''\n",
    "        \n",
    "        emp_feat_count=np.zeros(len(self.feature_indices) ) \n",
    "        count1=0 # this is for w,l\n",
    "        count2=0 # this is for pl,l\n",
    "\n",
    "        for sentence in self.train_sentences:\n",
    "            \n",
    "            for i, (word_i, label_i) in enumerate (sentence):\n",
    "                prev_label_i= 'START' if i==0 else sentence[i-1][1]\n",
    "\n",
    "                if word_i==word and label_i==label:\n",
    "                    count1+=1\n",
    "                \n",
    "                if label_i==label and prev_label_i==prev_label:\n",
    "                    count2+=1\n",
    "\n",
    "            index1=self.feature_indices[(word),(label)]\n",
    "            emp_feat_count[index1]=count1\n",
    "\n",
    "            index2=self.feature_indices[(prev_label),(label)]\n",
    "            emp_feat_count[index2]=count2\n",
    "\n",
    "\n",
    "        return emp_feat_count/self.train_size\n",
    "\n",
    "            \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # Exercise 3 b) ###################################################################\n",
    "    def expected_feature_count(self, word, prev_label):\n",
    "        '''\n",
    "        Compute the expected feature count given a word, the label of the previous word and the parameters of the current model\n",
    "        (see variable theta)\n",
    "        Parameters: word: string; a word x_i some position i of a given sentence\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: (numpy) array containing the expected feature count\n",
    "        '''\n",
    "    \n",
    "        exp_feat_count= np.zeros(len(self.feature_indices)) # final vector\n",
    "\n",
    "        for label in self.labels:\n",
    "            cond_p=self.conditional_probability(word, label, prev_label)\n",
    "            af= self.get_active_features(word, label, prev_label) # current active feats\n",
    "      \n",
    "            exp_feat_count+= (cond_p*af) # update for all labels \n",
    "\n",
    "        return exp_feat_count/self.train_size\n",
    "    \n",
    "\n",
    "    def parameter_update(self, word, label, prev_label, learning_rate=0.1):\n",
    "        '''\n",
    "        Do one learning step.\n",
    "        Parameters: word: string; a randomly selected word x_i at some position i of a given sentence\n",
    "                    label: string; the actual label of the selected word\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "                    learning_rate: float\n",
    "        '''\n",
    "        \n",
    "        new_theta=self.theta+learning_rate*(self.empirical_feature_count(word,label,prev_label)\n",
    "                                            - self.expected_feature_count(word, prev_label))\n",
    "    \n",
    "        self.theta=new_theta\n",
    "\n",
    "    def train(self, number_iterations, learning_rate=0.1):\n",
    "        '''\n",
    "        Implement the training procedure.\n",
    "        Parameters: number_iterations: int; number of parameter updates to do\n",
    "                    learning_rate: float\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        for _ in range (number_iterations):\n",
    "            sentence=random.choice(self.train_sentences)\n",
    "            \n",
    "            i,(word,label)=random.choice(list(enumerate(sentence)))\n",
    "            prev_label='START' if i==0 else sentence[i-1][1] \n",
    "            self.words_used.append(i+1)\n",
    "            self.parameter_update(word, label, prev_label)\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Exercise 4 c) ###################################################################\n",
    "    def predict(self, word, prev_label):\n",
    "        '''\n",
    "        Predict the most probable label of the word referenced by 'word'\n",
    "        Parameters: word: string; a word x_i at some position i of a given sentence\n",
    "                    prev_label: string; the label of the word at position i-1\n",
    "        Returns: string; most probable label\n",
    "        '''\n",
    "        preds=[]\n",
    "        \n",
    "        for label in self.labels:\n",
    "            prob=self.conditional_probability(word, label, prev_label)\n",
    "            preds.append((label, prob))\n",
    "\n",
    "        preds.sort(reverse=True)\n",
    "\n",
    "\n",
    "        return preds[0] \n",
    "\n",
    "    \n",
    "    def empirical_feature_count_batch(self, sentences):\n",
    "        '''\n",
    "        Predict the empirical feature count for a set of sentences\n",
    "        Parameters: sentences: list; a list of sentences; should be a sublist of the list returnd by 'import_corpus'\n",
    "        Returns: (numpy) array containing the empirical feature count\n",
    "        '''\n",
    "        \n",
    "        emp_feat_count=np.zeros(len(self.feature_indices))\n",
    "\n",
    "        for sentence in sentences:\n",
    "            if self.iter==0:\n",
    "                self.words_used.append(len(sentence))\n",
    "            else:\n",
    "                self.words_used.append(self.words_used[-1] + len(sentence))  \n",
    "            \n",
    "            for i, (word,label) in enumerate(sentence):\n",
    "                prev_label='START' if i==0 else sentence[i-1][1]\n",
    "                emp_feat_count+=self.empirical_feature_count(word, label, prev_label)\n",
    "        \n",
    "        return emp_feat_count/self.train_size\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Exercise 5 a) ###################################################################\n",
    "    def expected_feature_count_batch(self, sentences):\n",
    "        '''\n",
    "        Predict the expected feature count for a set of sentences\n",
    "        Parameters: sentences: list; a list of sentences; should be a sublist of the list returnd by 'import_corpus'\n",
    "        Returns: (numpy) array containing the expected feature count\n",
    "        '''\n",
    "        \n",
    "        exp_feat_count=np.zeros(len(self.feature_indices))\n",
    "\n",
    "        for sentence in sentences:\n",
    "            for i, (word,label) in enumerate(sentence):\n",
    "                prev_label='START' if i==0 else sentence[i-1][1]\n",
    "                exp_feat_count+=self.expected_feature_count(word, prev_label)\n",
    "\n",
    "        return exp_feat_count/self.train_size\n",
    "    \n",
    "    \n",
    "    def train_batch(self, number_iterations, batch_size, learning_rate=0.1):\n",
    "        '''\n",
    "        Implement the training procedure which uses 'batch_size' sentences from to training corpus\n",
    "        to compute the gradient.\n",
    "        Parameters: number_iterations: int; number of parameter updates to do\n",
    "                    batch_size: int; number of sentences to use in each iteration\n",
    "                    learning_rate: float\n",
    "        '''\n",
    "        \n",
    "        sentences=random.choices(self.train_sentences, k=batch_size)\n",
    "\n",
    "        for i in range(number_iterations):\n",
    "            self.iter=i\n",
    "            new_theta=self.theta+ learning_rate*(self.empirical_feature_count_batch(sentences)\n",
    "                                                 -self.expected_feature_count_batch(sentences))\n",
    "            \n",
    "\n",
    "        self.theta=new_theta\n",
    "\n",
    "\n",
    "    def accuracy(self, test):\n",
    "\n",
    "        correct=0\n",
    "        total=0\n",
    "\n",
    "        for sentence in test:\n",
    "\n",
    "            for i, (word,true) in enumerate(sentence):\n",
    "                prev_label = \"START\" if i == 0 else sentence[i - 1][1]\n",
    "                pred=self.predict(word, prev_label)\n",
    "                total+=1\n",
    "                if pred==true:\n",
    "                    correct+=1\n",
    "\n",
    "        return correct/total\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def evaluate(self, test, num_iter, model_type):\n",
    "\n",
    "        accuracies=[]\n",
    "        \n",
    "        \n",
    "        if model_type == 'Normal':\n",
    "                # Train using train method\n",
    "                self.train(1)\n",
    "                accuracies.append(self.accuracy(test))\n",
    "        \n",
    "        elif model_type == 'Batch':\n",
    "                # Train using train_batch method with one sentence at a time\n",
    "            self.train_batch(1, 1)\n",
    "            accuracies.append(self.accuracy(test))\n",
    "\n",
    "\n",
    "        plt.plot(self.words_used, accuracies)\n",
    "        plt.xlabel('Accuracy')\n",
    "        plt.ylabel('N of words used')\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
